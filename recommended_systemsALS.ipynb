{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"recommended_systems(ALS).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"146gAUnA_kx__t9yqZgCunJFw6xa0mBmd","authorship_tag":"ABX9TyPMqjuVsd08IMa9vfdZe1ua"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# `1.` 패키지 임포트\n","\n"],"metadata":{"id":"s8wL_uHfLIoy"}},{"cell_type":"code","source":["from scipy.stats import chisquare\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"metadata":{"id":"EPD9jdmqLSZC","executionInfo":{"status":"ok","timestamp":1650469772115,"user_tz":-540,"elapsed":927,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# `2.` 데이터 로드"],"metadata":{"id":"ViFAVUBxLtzZ"}},{"cell_type":"code","source":["# 시트 순서 (전체 관광지[0]), (제주시, 추자면[1]), (초전읍, 구좌읍, 우도[2]), (한경면, 한림읍, 애월읍[3]), (서귀포시 남원읍[4]), (성산읍, 표선면[5]), (안덕면, 대정읍[6]) (테스트[7])\n","train_data = pd.read_excel('/content/drive/MyDrive/Project/여행 코스 추천/data/학습용 데이터/tourist_data_learning.xlsx', sheet_name=0)"],"metadata":{"id":"898Ml2fbLvZB","executionInfo":{"status":"ok","timestamp":1650469774733,"user_tz":-540,"elapsed":2621,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# `3.` 데이터 확인"],"metadata":{"id":"q_-na8avMIR5"}},{"cell_type":"code","source":["display(train_data)\n","print(train_data.info())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"j7cxubyyMKpA","executionInfo":{"status":"ok","timestamp":1650469774734,"user_tz":-540,"elapsed":24,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"b55ec253-ab1d-497e-bb89-9b5e6af8d589"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["      Unnamed: 0        관광 명소                                              주소  \\\n","0            0.0           우도                        제주특별자치도 제주시 우도면 삼양고수물길 1   \n","1            1.0        성산일출봉                     제주특별자치도 서귀포시 성산읍 일출로 284-12   \n","2            2.0        사려니숲길                     제주특별자치도 제주시 조천읍 교래리 산 137-1   \n","3            3.0        카멜리아힐                             제주 서귀포시 안덕면 병악로 166   \n","4            4.0       협재해수욕장                      제주특별자치도 제주시 한림읍 한림로 329-10   \n","...          ...          ...                                             ...   \n","1091      1091.0          효명사                    제주특별자치도 서귀포시 남원읍 516로 815-41   \n","1092      1092.0         후포해변                         제주특별자치도 제주시 추자면 대서5길 87   \n","1093      1093.0     훈데르트바서파크                     제주특별자치도 제주시 우도면 우도해안길 32-12   \n","1094      1094.0  휘닉스 르쏠레이테라피  제주특별자치도 서귀포시 성산읍 섭지코지로 107 휘닉스제주아일랜드리조트 블루동 1층   \n","1095      1095.0           휴림                        제주특별자치도 제주시 애월읍 광령남서길 40   \n","\n","       역사   자연   사진   도보   예술   육상   수상  ...  사계절  어린이   청년   중년   노년   혼자  \\\n","0     0.0  2.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0   \n","1     0.0  3.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n","2     0.0  3.0  0.0  1.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n","3     0.0  2.0  1.0  0.0  0.0  1.0  0.0  ...  0.0  1.0  0.0  0.0  0.0  0.0   \n","4     0.0  3.0  0.0  0.0  0.0  1.0  4.0  ...  0.0  1.0  0.0  0.0  0.0  0.0   \n","...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n","1091  1.0  1.0  1.0  0.0  0.0  1.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n","1092  0.0  2.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n","1093  1.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n","1094  0.0  0.0  0.0  0.0  0.0  2.0  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0   \n","1095  0.0  1.0  0.0  0.0  0.0  2.0  1.0  ...  0.0  1.0  0.0  0.0  0.0  0.0   \n","\n","       친구   커플   부모   가족  \n","0     0.0  0.0  0.0  0.0  \n","1     0.0  0.0  1.0  0.0  \n","2     0.0  1.0  0.0  0.0  \n","3     0.0  1.0  0.0  0.0  \n","4     0.0  0.0  0.0  0.0  \n","...   ...  ...  ...  ...  \n","1091  0.0  0.0  0.0  0.0  \n","1092  0.0  0.0  0.0  0.0  \n","1093  0.0  0.0  0.0  0.0  \n","1094  0.0  0.0  0.0  0.0  \n","1095  0.0  1.0  1.0  0.0  \n","\n","[1096 rows x 25 columns]"],"text/html":["\n","  <div id=\"df-99a71c40-5d8b-4b41-b38e-bd936530897e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>관광 명소</th>\n","      <th>주소</th>\n","      <th>역사</th>\n","      <th>자연</th>\n","      <th>사진</th>\n","      <th>도보</th>\n","      <th>예술</th>\n","      <th>육상</th>\n","      <th>수상</th>\n","      <th>...</th>\n","      <th>사계절</th>\n","      <th>어린이</th>\n","      <th>청년</th>\n","      <th>중년</th>\n","      <th>노년</th>\n","      <th>혼자</th>\n","      <th>친구</th>\n","      <th>커플</th>\n","      <th>부모</th>\n","      <th>가족</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.0</td>\n","      <td>우도</td>\n","      <td>제주특별자치도 제주시 우도면 삼양고수물길 1</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1.0</td>\n","      <td>성산일출봉</td>\n","      <td>제주특별자치도 서귀포시 성산읍 일출로 284-12</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2.0</td>\n","      <td>사려니숲길</td>\n","      <td>제주특별자치도 제주시 조천읍 교래리 산 137-1</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3.0</td>\n","      <td>카멜리아힐</td>\n","      <td>제주 서귀포시 안덕면 병악로 166</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4.0</td>\n","      <td>협재해수욕장</td>\n","      <td>제주특별자치도 제주시 한림읍 한림로 329-10</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>4.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1091</th>\n","      <td>1091.0</td>\n","      <td>효명사</td>\n","      <td>제주특별자치도 서귀포시 남원읍 516로 815-41</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1092</th>\n","      <td>1092.0</td>\n","      <td>후포해변</td>\n","      <td>제주특별자치도 제주시 추자면 대서5길 87</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1093</th>\n","      <td>1093.0</td>\n","      <td>훈데르트바서파크</td>\n","      <td>제주특별자치도 제주시 우도면 우도해안길 32-12</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1094</th>\n","      <td>1094.0</td>\n","      <td>휘닉스 르쏠레이테라피</td>\n","      <td>제주특별자치도 서귀포시 성산읍 섭지코지로 107 휘닉스제주아일랜드리조트 블루동 1층</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","    </tr>\n","    <tr>\n","      <th>1095</th>\n","      <td>1095.0</td>\n","      <td>휴림</td>\n","      <td>제주특별자치도 제주시 애월읍 광령남서길 40</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2.0</td>\n","      <td>1.0</td>\n","      <td>...</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1096 rows × 25 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-99a71c40-5d8b-4b41-b38e-bd936530897e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-99a71c40-5d8b-4b41-b38e-bd936530897e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-99a71c40-5d8b-4b41-b38e-bd936530897e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1096 entries, 0 to 1095\n","Data columns (total 25 columns):\n"," #   Column      Non-Null Count  Dtype  \n","---  ------      --------------  -----  \n"," 0   Unnamed: 0  1096 non-null   float64\n"," 1   관광 명소       1095 non-null   object \n"," 2   주소          1088 non-null   object \n"," 3   역사          1096 non-null   float64\n"," 4   자연          1096 non-null   float64\n"," 5   사진          1096 non-null   float64\n"," 6   도보          1096 non-null   float64\n"," 7   예술          1096 non-null   float64\n"," 8   육상          1096 non-null   float64\n"," 9   수상          1096 non-null   float64\n"," 10  공중          1096 non-null   float64\n"," 11  봄           1096 non-null   float64\n"," 12  여름          1096 non-null   float64\n"," 13  가을          1096 non-null   float64\n"," 14  겨울          1096 non-null   float64\n"," 15  사계절         1096 non-null   float64\n"," 16  어린이         1096 non-null   float64\n"," 17  청년          1096 non-null   float64\n"," 18  중년          1096 non-null   float64\n"," 19  노년          1096 non-null   float64\n"," 20  혼자          1096 non-null   float64\n"," 21  친구          1096 non-null   float64\n"," 22  커플          1096 non-null   float64\n"," 23  부모          1096 non-null   float64\n"," 24  가족          1096 non-null   float64\n","dtypes: float64(23), object(2)\n","memory usage: 214.2+ KB\n","None\n"]}]},{"cell_type":"markdown","source":["# `4.` ALS 구축 및 학습 [row version]"],"metadata":{"id":"s72fbxr0LgD6"}},{"cell_type":"markdown","source":["## `4-1.` 학습 파라미터 초기화"],"metadata":{"id":"TcmWAWQtLnZa"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"1O9MDHZcKWA3","executionInfo":{"status":"ok","timestamp":1650469774735,"user_tz":-540,"elapsed":23,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"outputs":[],"source":["# 학습 파라미터를 초기화 합니다. 초기값은 논문에서 가장 좋은 성능을 낸다는 값으로 설정합니다.\n","r_lambda = 40 # 데이터 정규화에 필요한 변수입니다.\n","nf = 200      # Matrix Factorization 학습 시에 정하는 임의의 차원 수이며 보통 50에서 200 사이로 설정합니다.\n","alpha = 40    # Confidence level(신뢰도)을 조정할때 쓰일 변수입니다.  "]},{"cell_type":"markdown","source":["## `4-2.`학습용 데이터 생성"],"metadata":{"id":"adxBCFeRKe1r"}},{"cell_type":"code","source":["# 학습시킬 태그 데이터 행렬을 생성합니다. 행(row)은 관광지 수, 열(column)은 태그의 개수가 됩니다.\n","R = train_data.iloc[:, 3:].values               # train_data의 모든 행의 4번째 열부터 마지막 열의 원소 값만 추출하여 R 변수에 저장합니다.\n","print('R.shape: {}'.format(R.shape))            # R 변수의 크기를 출력합니다.\n","print('-----R-----\\n{}\\n-----------'.format(R)) # R 변수의 입력값의 모양을 확인합니다.\n","print('R type: {}'.format(type(R)))             # R 변수의 타입을 확인합니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1i4w4-fKZTX","executionInfo":{"status":"ok","timestamp":1650469774736,"user_tz":-540,"elapsed":24,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"c15311eb-071f-43d3-e504-7742cc81692f"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["R.shape: (1096, 22)\n","-----R-----\n","[[0. 2. 1. ... 0. 0. 0.]\n"," [0. 3. 1. ... 0. 1. 0.]\n"," [0. 3. 0. ... 1. 0. 0.]\n"," ...\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 1. 0. ... 1. 1. 0.]]\n","-----------\n","R type: <class 'numpy.ndarray'>\n"]}]},{"cell_type":"markdown","source":["## `4-3.` 관광지 수와 태그 수의 Latent Factor Matrix 초기화"],"metadata":{"id":"ygVA8EPtUQOE"}},{"cell_type":"code","source":["# 아주 작은 랜덤 한 값들로 행렬의 값을 초기화시킵니다.\n","nu = R.shape[0] # 관광지의 개수\n","ni = R.shape[1] # 태그의 개수\n","\n","# 행렬을 확인합니다.\n","print('nu: {}'.format(nu))\n","print('ni: {}'.format(ni))\n","print('-'*100)\n","\n","# 행렬의 값을 아주 작은 값으로 초기화합니다.(rand: 0 ~ 1사이의 표준정규분포 난수 매트릭스를 생성합니다.)\n","X = np.random.rand(nu, nf) * 0.01 # 관광지 수 매트릭스\n","Y = np.random.rand(ni, nf) * 0.01 # 태그 수 매트릭스\n","\n","# 초기화된 매트릭스를 확인합니다.\n","print(X)\n","print('-'*100)\n","print(Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MrOCx5yWKgpe","executionInfo":{"status":"ok","timestamp":1650469774738,"user_tz":-540,"elapsed":23,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"99e7511c-e952-4bb0-85fb-694914e21552"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["nu: 1096\n","ni: 22\n","----------------------------------------------------------------------------------------------------\n","[[0.00855287 0.00960142 0.00110373 ... 0.00818761 0.00736823 0.00445072]\n"," [0.00780137 0.00571401 0.00374823 ... 0.00130279 0.00394875 0.00909948]\n"," [0.00727399 0.0062529  0.00491698 ... 0.00252021 0.00121951 0.0020621 ]\n"," ...\n"," [0.00296263 0.00163079 0.00498306 ... 0.00288991 0.00728484 0.00352084]\n"," [0.00415882 0.00539922 0.00464091 ... 0.00710362 0.00737585 0.00135974]\n"," [0.00398662 0.00925207 0.00792014 ... 0.00419347 0.00032613 0.00129123]]\n","----------------------------------------------------------------------------------------------------\n","[[8.45406435e-03 2.68661660e-03 3.21376713e-03 ... 9.68761272e-03\n","  5.98151441e-04 6.44994764e-03]\n"," [3.57832756e-03 3.89403876e-03 9.47593956e-03 ... 1.35974360e-03\n","  8.41444910e-03 4.74102233e-03]\n"," [1.14912705e-03 7.83199139e-03 2.92153854e-03 ... 3.20100386e-04\n","  4.99564497e-03 8.41176119e-03]\n"," ...\n"," [3.51528455e-04 2.07690532e-03 4.90687456e-03 ... 3.02086645e-03\n","  5.80258631e-03 5.23011824e-03]\n"," [2.19708343e-03 7.10698978e-03 4.10642864e-03 ... 1.25038350e-04\n","  4.62796092e-03 7.33113312e-03]\n"," [9.43168588e-05 3.82490167e-03 3.73363931e-03 ... 3.19834245e-03\n","  3.76898959e-03 8.35503121e-03]]\n"]}]},{"cell_type":"markdown","source":["## `4-4.` 선호도 설정 P"],"metadata":{"id":"xAxp7-7UKjpq"}},{"cell_type":"code","source":["# 주어진 학습용 태그 테이블을 0과 1로 된 binary rating matrix P로 바꾸어줍니다.\n","P = np.copy(R) # R 사본을 생성합니다.\n","P[P > 0] = 1   # 0보다 큰 값을 1로 치환하여 P 변수에 대입합니다.\n","print(P)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vODCdUYMKlCl","executionInfo":{"status":"ok","timestamp":1650469774739,"user_tz":-540,"elapsed":22,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"616be3e1-baf7-4a5c-8242-4798c0d6c78b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 1. 1. ... 0. 0. 0.]\n"," [0. 1. 1. ... 0. 1. 0.]\n"," [0. 1. 0. ... 1. 0. 0.]\n"," ...\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 1. 0. ... 1. 1. 0.]]\n"]}]},{"cell_type":"markdown","source":["## `4-5.` 신뢰도 설정 C"],"metadata":{"id":"Xa1xm7TXKmxS"}},{"cell_type":"code","source":["# 학습용 태그 테이블에 Confidence level을 적용한 C 행렬을 구합니다.\n","C = 1 + alpha * R # 1 + (40 * R)\n","print(C)\n","print('-'*100) \n","print('C.shape: {}'.format(C.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DitFDxApKlY9","executionInfo":{"status":"ok","timestamp":1650469774741,"user_tz":-540,"elapsed":21,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"09331ddb-4307-4013-e8ab-30bb84e1561b"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[[  1.  81.  41. ...   1.   1.   1.]\n"," [  1. 121.  41. ...   1.  41.   1.]\n"," [  1. 121.   1. ...  41.   1.   1.]\n"," ...\n"," [ 41.   1.   1. ...   1.   1.   1.]\n"," [  1.   1.   1. ...   1.   1.   1.]\n"," [  1.  41.   1. ...  41.  41.   1.]]\n","----------------------------------------------------------------------------------------------------\n","C.shape: (1096, 22)\n"]}]},{"cell_type":"markdown","source":["## `4-6.` Loss Function 구현\n","- C: 신뢰도 행렬(confidence matrix)\n","- P: 이항 정격 행렬(binary rating matrix)\n","- X: 관광지 수 행렬(tourist latent matrix)\n","- Y: 태그 수 행렬(tag latente matrix)\n","- r_lambda: 정규화 조절 값(regularization lambda)\n","- xTy: 예측 행렬(predicted matrix)\n","- total_loss = (confidence_level * predict_loss) + regularization_loss\n","![image.png](data:image/png;base64,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)"],"metadata":{"id":"dExK3e8bKpBk"}},{"cell_type":"code","source":["def loss_function(C, P, xTy, X, Y, r_lambda) :\n","  predict_error = np.square(R - xTy)                                         # R의 행렬 요소에 xTy행렬 요소를 각각 빼고 모든 요소를 제곱하여 predict_error 변수에 저장합니다.\n","  confidence_error = np.sum(C * predict_error)                               # C의 행렬 배열과 predict_error의 행렬 배열을 모두 합하여 confidence_error 변수에 저장합니다.\n","  regularzation = r_lambda * ((np.sum(np.square(X))) + np.sum(np.square(Y))) # X와 Y의 행렬 배열의 각 요소에 제곱을 한 뒤 모든 요소를 합하고 r_lambda(40)을 곱한 뒤 regularzation 변수에 저장합니다.\n","  total_loss = confidence_error + regularzation                              # confidence_error와 regularzation을 합하여 total_loss 변수에 저장합니다.\n","  return np.sum(predict_error), confidence_error, regularzation, total_loss"],"metadata":{"id":"sewj5dCqKpMV","executionInfo":{"status":"ok","timestamp":1650469774744,"user_tz":-540,"elapsed":22,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## `4-7.` Optimizer 설정\n","- X[u] = (yTCuy + lambda*I)^-1yTCuy\n","- Y[i] = (xTCix + lambda*I)^-1xTCix"],"metadata":{"id":"1inmFeEagHxY"}},{"cell_type":"code","source":["# 로스 펑션을 최적화 시키는 Optimizer를 작성합니다.\n","def optimize_tourist(X, Y, C, P, nu, nf, r_lambda) :\n","  yT = np.transpose(Y) # Y에 대한 전치 행렬을 구하여 yT 변수에 저장합니다.\n","\n","  for u in range(nu) :                             # nu 수만큼 반복합니다. (1,096)\n","    Cu = np.diag(C[u])                             # C의 u번째 대각 원소를 추출하고 대각 행렬을 만들어내어 Cu 변수에 저장합니다.\n","    yT_Cu_y = np.matmul(np.matmul(yT, Cu), Y)      # yT, Cu 변수의 행렬 곱을 수행하고 난 뒤 결괏값과 Y 변수의 행렬 곱을 수행하고 yT_Cu_y 변수에 저장합니다.\n","    lI = np.dot(r_lambda, np.identity(nf))         # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","    yT_Cu_pu = np.matmul(np.matmul(yT, Cu), P[u])  # yT, Cu 배열의 행렬 곱을 수행한 결괏값과 P[u] 배열의 행렬 곱을 수행하고 yT_Cu_pu 변수에 저장합니다.\n","    X[u] = np.linalg.solve(yT_Cu_y + lI, yT_Cu_pu) # yT_Cu_y + lI 결괏값과 yT_Cu_pu의 연립방정식을 풉니다.\n","\n","def optimize_tag(X, Y, C, P, ni, nf, r_lambda) :\n","  xT = np.transpose(X) # X에 대한 전치 행렬을 구하여 xT 변수에 저장합니다.\n"," \n","  for i in range(ni) :                               # ni 수만큼 반복합니다. (22)\n","    Ci = np.diag(C[:, i])                            # C의 모든 행의 I번째 컬럼을 추출하여 Ci 변수에 저장합니다.\n","    xT_Ci_x = np.matmul(np.matmul(xT, Ci), X)        # xT, Ci의 행렬 곱을 수행한 결괏값과 X와 행렬 곱을 수행하고 xT_Ci_x 변수에 저장합니다.\n","    lI = np.dot(r_lambda, np.identity(nf))           # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","    xT_Ci_pi = np.matmul(np.matmul(xT, Ci), P[:, i]) # xT, Ci의 행렬 곱을 수행한 결괏값과 P의 모든 행의 i번째 컬럼 원소값과 행렬곱을 수행하고 xT_Ci_pi 변수에 저장합니다.\n","    Y[i] = np.linalg.solve(xT_Ci_x + lI, xT_Ci_pi)   # xT_Ci_x + lI 결괏값과 xT_Ci_pi 연립방정식을 풉니다."],"metadata":{"id":"fDYVRa08gK-e","executionInfo":{"status":"ok","timestamp":1650469774745,"user_tz":-540,"elapsed":22,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["## `4-8.` ALS 학습"],"metadata":{"id":"njmqf1fSKw2a"}},{"cell_type":"code","source":["predict_errors = []\n","confidence_errors = []\n","regularization_list = []\n","total_losses = []\n","\n","for i in range(17) :\n","  \n","  if i!=0 :\n","    optimize_tourist(X, Y, C, P, nu, nf, r_lambda)\n","    optimize_tag(X, Y, C, P, ni, nf, r_lambda)\n","  \n","  predict = np.matmul(X, np.transpose(Y))\n","  predict_error, confidence_error, regularization, total_loss = loss_function(C, P, predict, X, Y, r_lambda)\n","  \n","  predict_errors.append(predict_error)\n","  confidence_errors.append(confidence_error)\n","  regularization_list.append(regularization)\n","  total_losses.append(total_loss)\n","\n","  print('--------------------step %d------------------------------' % i)\n","  print('predict error: %f' % predict_error)\n","  print('confidence error: %f' % confidence_error)\n","  print('regularization: %f' % regularization)\n","  print('total loss: %f' % total_loss)\n","\n","predict = np.matmul(X, np.transpose(Y))\n","print('-'*100)\n","print('final predict')\n","print([predict])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HIhojBYK0CJ","executionInfo":{"status":"ok","timestamp":1650469837325,"user_tz":-540,"elapsed":62602,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"2e9363b6-aa31-4384-960c-389bd1112605"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["--------------------step 0------------------------------\n","predict error: 7980.401719\n","confidence error: 628044.445812\n","regularization: 297.886679\n","total loss: 628342.332491\n","--------------------step 1------------------------------\n","predict error: 7043.601804\n","confidence error: 211344.009097\n","regularization: 12217.097142\n","total loss: 223561.106239\n","--------------------step 2------------------------------\n","predict error: 3533.279300\n","confidence error: 195534.315195\n","regularization: 13754.690608\n","total loss: 209289.005802\n","--------------------step 3------------------------------\n","predict error: 3120.145187\n","confidence error: 194842.995775\n","regularization: 13813.004885\n","total loss: 208656.000659\n","--------------------step 4------------------------------\n","predict error: 3068.291616\n","confidence error: 194657.418878\n","regularization: 13718.502043\n","total loss: 208375.920920\n","--------------------step 5------------------------------\n","predict error: 3072.574957\n","confidence error: 194593.078618\n","regularization: 13626.570592\n","total loss: 208219.649210\n","--------------------step 6------------------------------\n","predict error: 3090.046963\n","confidence error: 194568.033141\n","regularization: 13552.974927\n","total loss: 208121.008068\n","--------------------step 7------------------------------\n","predict error: 3109.160356\n","confidence error: 194556.988345\n","regularization: 13496.425164\n","total loss: 208053.413510\n","--------------------step 8------------------------------\n","predict error: 3126.434798\n","confidence error: 194551.081872\n","regularization: 13453.455641\n","total loss: 208004.537513\n","--------------------step 9------------------------------\n","predict error: 3140.987859\n","confidence error: 194546.971306\n","regularization: 13420.886894\n","total loss: 207967.858201\n","--------------------step 10------------------------------\n","predict error: 3152.845749\n","confidence error: 194543.402926\n","regularization: 13396.182253\n","total loss: 207939.585179\n","--------------------step 11------------------------------\n","predict error: 3162.337366\n","confidence error: 194539.954519\n","regularization: 13377.400220\n","total loss: 207917.354739\n","--------------------step 12------------------------------\n","predict error: 3169.860106\n","confidence error: 194536.531799\n","regularization: 13363.077469\n","total loss: 207899.609268\n","--------------------step 13------------------------------\n","predict error: 3175.789966\n","confidence error: 194533.158997\n","regularization: 13352.117388\n","total loss: 207885.276385\n","--------------------step 14------------------------------\n","predict error: 3180.451360\n","confidence error: 194529.892141\n","regularization: 13343.699359\n","total loss: 207873.591500\n","--------------------step 15------------------------------\n","predict error: 3184.111840\n","confidence error: 194526.785078\n","regularization: 13337.208832\n","total loss: 207863.993910\n","--------------------step 16------------------------------\n","predict error: 3186.986659\n","confidence error: 194523.878213\n","regularization: 13332.184664\n","total loss: 207856.062877\n","----------------------------------------------------------------------------------------------------\n","final predict\n","[array([[ 0.28913225,  0.98952251,  0.93521896, ...,  0.53247332,\n","         0.52045459,  0.01981057],\n","       [ 0.21451993,  0.98782469,  0.93613882, ...,  0.62345389,\n","         0.88185286,  0.07869176],\n","       [ 0.40957748,  0.99907005,  0.46652394, ...,  0.94594662,\n","         0.28434823, -0.0205566 ],\n","       ...,\n","       [ 0.91533342,  0.23680519, -0.02342157, ...,  0.02677029,\n","         0.06859068, -0.01216234],\n","       [ 0.36693837,  0.31042597,  0.03261425, ...,  0.20210822,\n","         0.06978259,  0.07202864],\n","       [ 0.195745  ,  0.96935933,  0.46558813, ...,  0.98174183,\n","         0.91225615,  0.132186  ]])]\n"]}]},{"cell_type":"markdown","source":["# 모델 생성 - ① 태그 카운팅 학습 데이터 모델\n","- ASL 알고리즘을 응용하여 관광지를 추천하는 모델을 생성합니다.\n","- my_id, my_vector = input_user_id, input_user_vector\n","- user_id, user_vector =  train_id, train_vector "],"metadata":{"id":"hSA6n9OUL0Vz"}},{"cell_type":"code","source":["def RecommendedSystems(area_code, input_data) :\n","\n","  # 필요한 패키지를 임포트합니다.\n","  import pandas as pd\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","\n","  ############################################################\n","  # 코사인 유사도를 구하는 함수를 생성합니다.\n","  def CosineSimilarity(v1, v2) :\n","    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","  # Loss Function을 구현합니다.\n","  def LossFunction(C, R, predict, X, Y, r_lambda) :\n","    predict_error = np.square(R - predict)                                     # R의 행렬 요소에 predict행렬 요소를 각각 빼고 모든 요소를 제곱하여 predict_error 변수에 저장합니다.\n","    confidence_error = np.sum(C * predict_error)                               # C의 행렬 배열과 predict_error의 행렬 배열을 모두 합하여 confidence_error 변수에 저장합니다.\n","    regularzation = r_lambda * ((np.sum(np.square(X))) + np.sum(np.square(Y))) # X와 Y의 행렬 배열의 각 요소에 제곱을 한 뒤 모든 요소를 합하고 r_lambda(40)을 곱한 뒤 regularzation 변수에 저장합니다.\n","    total_loss = confidence_error + regularzation                              # confidence_error와 regularzation을 합하여 total_loss 변수에 저장합니다.\n","    return np.sum(predict_error), confidence_error, regularzation, total_loss\n","\n","  # Loss Function을 최적화 시키는 Optimizer를 생성합니다.\n","  def OptimizeTourist(X, Y, C, R, nu, nf, r_lambda) :  \n","    yT = np.transpose(Y) # Y에 대한 전치 행렬을 구하여 yT 변수에 저장합니다.\n","\n","    for u in range(nu) :                             # nu 수만큼 반복합니다. (1,096)\n","      Cu = np.diag(C[u])                             # C의 u번째 대각 원소를 추출하고 대각 행렬을 만들어내어 Cu 변수에 저장합니다.\n","      yT_Cu_y = np.matmul(np.matmul(yT, Cu), Y)      # yT, Cu 변수의 행렬 곱을 수행하고 난 뒤 결괏값과 Y 변수의 행렬 곱을 수행하고 yT_Cu_y 변수에 저장합니다.\n","      lI = np.dot(r_lambda, np.identity(nf))         # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","      yT_Cu_pu = np.matmul(np.matmul(yT, Cu), R[u])  # yT, Cu 배열의 행렬 곱을 수행한 결괏값과 P[u] 배열의 행렬 곱을 수행하고 yT_Cu_pu 변수에 저장합니다.\n","      X[u] = np.linalg.solve(yT_Cu_y + lI, yT_Cu_pu) # yT_Cu_y + lI 결괏값과 yT_Cu_pu의 연립방정식을 풉니다.\n","\n","  def OptimizeTag(X, Y, C, R, ni, nf, r_lambda) :\n","    xT = np.transpose(X) # X에 대한 전치 행렬을 구하여 xT 변수에 저장합니다.\n","  \n","    for i in range(ni) :                               # ni 수만큼 반복합니다. (22)\n","      Ci = np.diag(C[:, i])                            # C의 모든 행의 I번째 컬럼을 추출하여 Ci 변수에 저장합니다.\n","      xT_Ci_x = np.matmul(np.matmul(xT, Ci), X)        # xT, Ci의 행렬 곱을 수행한 결괏값과 X와 행렬 곱을 수행하고 xT_Ci_x 변수에 저장합니다.\n","      lI = np.dot(r_lambda, np.identity(nf))           # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","      xT_Ci_pi = np.matmul(np.matmul(xT, Ci), R[:, i]) # xT, Ci의 행렬 곱을 수행한 결괏값과 P의 모든 행의 i번째 컬럼 원소값과 행렬곱을 수행하고 xT_Ci_pi 변수에 저장합니다.\n","      Y[i] = np.linalg.solve(xT_Ci_x + lI, xT_Ci_pi)   # xT_Ci_x + lI 결괏값과 xT_Ci_pi 연립방정식을 풉니다.\n","\n","  ############################################################\n","\n","  # 시트 순서 (전체 관광지[0]), (제주시, 추자면[1]), (초전읍, 구좌읍, 우도[2]), (한경면, 한림읍, 애월읍[3]), (서귀포시 남원읍[4]), (성산읍, 표선면[5]), (안덕면, 대정읍[6]) (테스트[7])\n","  df = pd.read_excel('/content/drive/MyDrive/Project/여행 코스 추천/data/학습용 데이터/tourist_data_learning.xlsx', index_col=0, sheet_name=area_code)\n","\n","  # 사본을 생성합니다.\n","  df_copy = df.copy()\n","\n","  # 필요없는 컬럼을 제거합니다.\n","  df.drop(['주소', '관광 명소'], axis=1, inplace=True)\n","\n","  # 관광지 정보 데이터와 사용자로 하여금 입력받은 데이터를 이어붙입니다.\n","  train_data = pd.concat([df, input_data])\n","\n","  # 학습 파라미터를 초기화 합니다. 초기값은 논문에서 가장 좋은 성능을 낸다는 값으로 설정합니다.\n","  r_lambda = 40 # 데이터 정규화에 필요한 변수입니다.\n","  nf = 200      # Matrix Factorization 학습 시에 정하는 임의의 차원 수이며 보통 50에서 200 사이로 설정합니다.\n","  alpha = 40    # Confidence level(신뢰도)을 조정할때 쓰일 변수입니다.\n","\n","  # 학습시킬 태그 데이터 행렬을 생성합니다. 행(row)은 관광지 수, 열(column)은 태그의 개수가 됩니다.\n","  R = train_data.iloc[:, 1:].values # train_data의 모든 행의 2번째 열부터 마지막 열의 원소 값만 추출하여 R 변수에 저장합니다.\n","\n","  # 아주 작은 랜덤 한 값들로 행렬의 값을 초기화시킵니다.\n","  nu = R.shape[0] # 관광지의 개수\n","  ni = R.shape[1] # 태그의 개수\n","\n","  # 행렬의 값을 아주 작은 값으로 초기화합니다.(rand: 0 ~ 1사이의 표준정규분포 난수 매트릭스를 생성합니다.)\n","  X = np.random.rand(nu, nf) * 0.01 # 관광지 수 매트릭스\n","  Y = np.random.rand(ni, nf) * 0.01 # 태그 수 매트릭스\n","\n","  # 학습용 태그 테이블에 Confidence level을 적용한 C 행렬을 구합니다.\n","  C = 1 + alpha * R\n","\n","  # 가중치를 조절하며 loss가 수렴되는 최종 ALS 행렬을 구합니다.\n","  for i in range(17) :\n","  \n","    if i != 0 :\n","      OptimizeTourist(X, Y, C, R, nu, nf, r_lambda) # optimize_tourist 함수를 불러와 loss를 구하고 가중치를 조절합니다.\n","      OptimizeTag(X, Y, C, R, ni, nf, r_lambda)     # optimize_tag 함수를 불러와 loss를 구하고 가중치를 조절합니다.\n","  \n","    predict = np.matmul(X, np.transpose(Y))\n","\n","  predict = np.matmul(X, np.transpose(Y))\n","\n","  # 코사인 유사도 함수를 활용하여 ALS 알고리즘을 거친 기본 데이터와 입력 데이터의 행렬을 나누어 입력 데이터 행렬에 가장 유사한 값을 기본 데이터 행렬에서 찾아 추출합니다.\n","  input_user_id = predict.shape[0] - 1         # predict.shape ex: (206, 22) [0] 번째에 해당하는 206에서 1을 뺀 값을 input_id 변수에 넘겨줍니다.(인덱싱에서 1번째 행은 0번 인덱스 번호를 가지기 때문)\n","  input_user_vector = predict[[input_user_id]] # predict행렬에 input_id번째 행렬 배열을 input_vector 변수에 전달합니다.\n","  best_match_score = -1                        # 코사인 유사도는 -1 ~ 1 사이의 값을 가지므로 최소값인 -1로 변수를 초기화합니다.\n","  best_match_id = -1                           # 코사인 유사도는 -1 ~ 1 사이의 값을 가지므로 최소값인 -1로 변수를 초기화합니다.\n","  best_match_matrix = []                       # 코사인 유사도로 가장 유사한 데이터의 행렬을 받을 변수입니다.\n","  cos_similarity_matrix = []                   # 입력 데이터와 기본 데이터 행렬의 코사인 유사도와 인덱스 번호를 받을 리스트 변수입니다.\n","\n","  # 반복문을 통하여 입력 데이터 행렬을 기본 데이터 행렬과 비교하여 가장 유사한 값을 찾습니다.\n","  for train_id, train_vector in enumerate(predict) :                     # 기본 데이터와 입력 데이터를 합쳐서 ALS 알고리즘을 거친 행렬에서 각각의 원소 행렬 하나씩을 추출하여 반복합니다.\n","    if input_user_id != train_id :                                       # 입력 데이터 인덱스 번호가 기본 데이터 인덱스 번호와 맞지 않다면(predict의 마지막에는 입력값이 존재하기 때문에 중복을 방지하기 위함)\n","      cos_similarity = CosineSimilarity(input_user_vector, train_vector) # 입력 데이터 행렬과 기본 데이터 행렬의 코사인 유사도를 구하고 cos_similarity 변수에 대입합니다.\n","      if cos_similarity > -1 :                                           # 코사인 유사도가 -1보다 크다면\n","        cos_similarity_matrix.append([cos_similarity, train_id])         # 코사인 유사도와 해당 인덱스 번호를 cos_similarity_matrix 변수에 추가합니다.\n","\n","  # 코사인 유사도를 기준으로 재정렬 한 뒤 상위 n개의 인덱스 번호를 추출합니다.\n","  cos_similarity_matrix.sort(reverse=True)                 # 코사인 유사도를 기준으로 내림차순으로 정렬합니다.\n","  arr_predict = [i[1] for i in cos_similarity_matrix[:10]] # cos_similarity_matrix 변수에 i 번째 리스트에서 1번째 인덱스의 값을 추출하여 arr_predict 변수에 저장합니다.\n","  arr_predict.insert(0, R.shape[0]-1)                      # arr_predict 리스트 0번째 자리에 R의 행 크기의 -1을 뺀 값 즉, 마지막 데이터의 인덱스 번호를 삽입합니다.\n","\n","  # 내적 연산은 통해 사용자가 선택한 테마에 태그가 더 많은 장소를 우선적으로 추천합니다\n","  input_user_id = arr_predict[0]                            # 입력값의 인덱스 번호를 input_id 변수에 넘겨줍니다.\n","  input_user_vector = predict[arr_predict[0]]               # 입력값의 ALS 행렬을 input_vector 변수에 넘겨줍니다.\n","  best_match, best_match_id, best_match_vector = -1, -1, [] \n","  similar_user_dot_predict = []                             # 내적 연산을 수행한 뒤 행렬과 인덱스 번호를 받을 리스트 변수입니다.\n","  \n","  for x in arr_predict :                                        # arr_predict 변수에 저장된 인덱스 번호 개수 만큼 반복합니다.\n","    train_id = x                                                # arr_predict 변수에 저장된 인덱스 번호를 train_id 변수에 저장합니다.\n","    train_vector = predict[x]                                   # predict에 x번째 행렬값을 train_vector 변수에 넘겨줍니다.\n","    if input_user_id != train_id :                              # 입력값 인덱스 번호가 기본 데이터 인덱스 번호와 같지 않다면(중복 방지)\n","      similarity = np.dot(input_user_vector, train_vector)      # 내적 연산을 수행합니다.\n","      if similarity > 0 :                                       # 내적 연산을 수행한 값이 0보다 크다면\n","        similar_user_dot_predict.append([similarity, train_id]) # similar_user_dot_predict 변수에 내적 연산을 수행한 결괏값과 기본 데이터의 인덱스 번호를 추가합니다.\n","\n","  similar_user_dot_predict.sort(reverse=True)        # 내적 연산을 수행항 값을 기준으로 내림차순으로 정렬합니다.\n","  arr_dot = [i[1] for i in similar_user_dot_predict] # 인덱스 번호를 추출하여 arr_dot 리스트 변수에 넘겨줍니다.\n","\n","  # 추출한 인덱스 번호를 이용하여 '관광 명소'와 '주소'를 출력으로 내보냅니다.\n","  result = df_copy.iloc[arr_dot, 0:2]\n","\n","  return result"],"metadata":{"id":"MMqFUCo6GxPe","executionInfo":{"status":"ok","timestamp":1650469837875,"user_tz":-540,"elapsed":554,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["# 모델 생성 - ② 태그 all 1 학습 데이터 모델\n","- ASL 알고리즘을 응용하여 관광지를 추천하는 모델을 생성합니다.\n","- 모든 태그의 수를 0이상의 값은 1로 바꾸어 확인합니다.\n","- 내적 연산은 하지 않습니다.\n","- my_id, my_vector = input_user_id, input_user_vector\n","- user_id, user_vector =  train_id, train_vector "],"metadata":{"id":"50XScbB-NUIQ"}},{"cell_type":"code","source":["def RecommendedSystems1(area_code, input_data) :\n","\n","  # 필요한 패키지를 임포트합니다.\n","  import pandas as pd\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","\n","  ############################################################\n","  # 코사인 유사도를 구하는 함수를 생성합니다.\n","  def CosineSimilarity(v1, v2) :\n","    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","  # Loss Function을 구현합니다.\n","  def LossFunction(C, R, predict, X, Y, r_lambda) :\n","    predict_error = np.square(R - predict)                                     # R의 행렬 요소에 predict행렬 요소를 각각 빼고 모든 요소를 제곱하여 predict_error 변수에 저장합니다.\n","    confidence_error = np.sum(C * predict_error)                               # C의 행렬 배열과 predict_error의 행렬 배열을 모두 합하여 confidence_error 변수에 저장합니다.\n","    regularzation = r_lambda * ((np.sum(np.square(X))) + np.sum(np.square(Y))) # X와 Y의 행렬 배열의 각 요소에 제곱을 한 뒤 모든 요소를 합하고 r_lambda(40)을 곱한 뒤 regularzation 변수에 저장합니다.\n","    total_loss = confidence_error + regularzation                              # confidence_error와 regularzation을 합하여 total_loss 변수에 저장합니다.\n","    return np.sum(predict_error), confidence_error, regularzation, total_loss\n","\n","  # Loss Function을 최적화 시키는 Optimizer를 생성합니다.\n","  def OptimizeTourist(X, Y, C, R, nu, nf, r_lambda) :  \n","    yT = np.transpose(Y) # Y에 대한 전치 행렬을 구하여 yT 변수에 저장합니다.\n","\n","    for u in range(nu) :                             # nu 수만큼 반복합니다. (1,096)\n","      Cu = np.diag(C[u])                             # C의 u번째 대각 원소를 추출하고 대각 행렬을 만들어내어 Cu 변수에 저장합니다.\n","      yT_Cu_y = np.matmul(np.matmul(yT, Cu), Y)      # yT, Cu 변수의 행렬 곱을 수행하고 난 뒤 결괏값과 Y 변수의 행렬 곱을 수행하고 yT_Cu_y 변수에 저장합니다.\n","      lI = np.dot(r_lambda, np.identity(nf))         # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","      yT_Cu_pu = np.matmul(np.matmul(yT, Cu), R[u])  # yT, Cu 배열의 행렬 곱을 수행한 결괏값과 P[u] 배열의 행렬 곱을 수행하고 yT_Cu_pu 변수에 저장합니다.\n","      X[u] = np.linalg.solve(yT_Cu_y + lI, yT_Cu_pu) # yT_Cu_y + lI 결괏값과 yT_Cu_pu의 연립방정식을 풉니다.\n","\n","  def OptimizeTag(X, Y, C, R, ni, nf, r_lambda) :\n","    xT = np.transpose(X) # X에 대한 전치 행렬을 구하여 xT 변수에 저장합니다.\n","  \n","    for i in range(ni) :                               # ni 수만큼 반복합니다. (22)\n","      Ci = np.diag(C[:, i])                            # C의 모든 행의 I번째 컬럼을 추출하여 Ci 변수에 저장합니다.\n","      xT_Ci_x = np.matmul(np.matmul(xT, Ci), X)        # xT, Ci의 행렬 곱을 수행한 결괏값과 X와 행렬 곱을 수행하고 xT_Ci_x 변수에 저장합니다.\n","      lI = np.dot(r_lambda, np.identity(nf))           # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","      xT_Ci_pi = np.matmul(np.matmul(xT, Ci), R[:, i]) # xT, Ci의 행렬 곱을 수행한 결괏값과 P의 모든 행의 i번째 컬럼 원소값과 행렬곱을 수행하고 xT_Ci_pi 변수에 저장합니다.\n","      Y[i] = np.linalg.solve(xT_Ci_x + lI, xT_Ci_pi)   # xT_Ci_x + lI 결괏값과 xT_Ci_pi 연립방정식을 풉니다.\n","\n","  ############################################################\n","\n","  # 시트 순서 (전체 관광지[0]), (제주시, 추자면[1]), (초전읍, 구좌읍, 우도[2]), (한경면, 한림읍, 애월읍[3]), (서귀포시 남원읍[4]), (성산읍, 표선면[5]), (안덕면, 대정읍[6]) (테스트[7])\n","  df = pd.read_excel('/content/drive/MyDrive/Project/여행 코스 추천/data/학습용 데이터/tourist_data_learning(all_1).xlsx', index_col=0, sheet_name=area_code)\n","\n","  # 사본을 생성합니다.\n","  df_copy = df.copy()\n","\n","  # 필요없는 컬럼을 제거합니다.\n","  df.drop(['주소', '관광 명소'], axis=1, inplace=True)\n","\n","  # 관광지 정보 데이터와 사용자로 하여금 입력받은 데이터를 이어붙입니다.\n","  train_data = pd.concat([df, input_data])\n","\n","  # 학습 파라미터를 초기화 합니다. 초기값은 논문에서 가장 좋은 성능을 낸다는 값으로 설정합니다.\n","  r_lambda = 40 # 데이터 정규화에 필요한 변수입니다.\n","  nf = 200      # Matrix Factorization 학습 시에 정하는 임의의 차원 수이며 보통 50에서 200 사이로 설정합니다.\n","  alpha = 40    # Confidence level(신뢰도)을 조정할때 쓰일 변수입니다.\n","\n","  # 학습시킬 태그 데이터 행렬을 생성합니다. 행(row)은 관광지 수, 열(column)은 태그의 개수가 됩니다.\n","  R = train_data.iloc[:, 1:].values # train_data의 모든 행의 2번째 열부터 마지막 열의 원소 값만 추출하여 R 변수에 저장합니다.\n","\n","  # 아주 작은 랜덤 한 값들로 행렬의 값을 초기화시킵니다.\n","  nu = R.shape[0] # 관광지의 개수\n","  ni = R.shape[1] # 태그의 개수\n","\n","  # 행렬의 값을 아주 작은 값으로 초기화합니다.(rand: 0 ~ 1사이의 표준정규분포 난수 매트릭스를 생성합니다.)\n","  X = np.random.rand(nu, nf) * 0.01 # 관광지 수 매트릭스\n","  Y = np.random.rand(ni, nf) * 0.01 # 태그 수 매트릭스\n","\n","  # 학습용 태그 테이블에 Confidence level을 적용한 C 행렬을 구합니다.\n","  C = 1 + alpha * R\n","\n","  # 가중치를 조절하며 loss가 수렴되는 최종 ALS 행렬을 구합니다.\n","  for i in range(17) :\n","  \n","    if i != 0 :\n","      OptimizeTourist(X, Y, C, R, nu, nf, r_lambda) # optimize_tourist 함수를 불러와 loss를 구하고 가중치를 조절합니다.\n","      OptimizeTag(X, Y, C, R, ni, nf, r_lambda)     # optimize_tag 함수를 불러와 loss를 구하고 가중치를 조절합니다.\n","  \n","    predict = np.matmul(X, np.transpose(Y))\n","\n","  predict = np.matmul(X, np.transpose(Y))\n","\n","  # 코사인 유사도 함수를 활용하여 ALS 알고리즘을 거친 기본 데이터와 입력 데이터의 행렬을 나누어 입력 데이터 행렬에 가장 유사한 값을 기본 데이터 행렬에서 찾아 추출합니다.\n","  input_user_id = predict.shape[0] - 1         # predict.shape ex: (206, 22) [0] 번째에 해당하는 206에서 1을 뺀 값을 input_id 변수에 넘겨줍니다.(인덱싱에서 1번째 행은 0번 인덱스 번호를 가지기 때문)\n","  input_user_vector = predict[[input_user_id]] # predict행렬에 input_id번째 행렬 배열을 input_vector 변수에 전달합니다.\n","  best_match_score = -1                        # 코사인 유사도는 -1 ~ 1 사이의 값을 가지므로 최소값인 -1로 변수를 초기화합니다.\n","  best_match_id = -1                           # 코사인 유사도는 -1 ~ 1 사이의 값을 가지므로 최소값인 -1로 변수를 초기화합니다.\n","  best_match_matrix = []                       # 코사인 유사도로 가장 유사한 데이터의 행렬을 받을 변수입니다.\n","  cos_similarity_matrix = []                   # 입력 데이터와 기본 데이터 행렬의 코사인 유사도와 인덱스 번호를 받을 리스트 변수입니다.\n","\n","  # 반복문을 통하여 입력 데이터 행렬을 기본 데이터 행렬과 비교하여 가장 유사한 값을 찾습니다.\n","  for train_id, train_vector in enumerate(predict) :                     # 기본 데이터와 입력 데이터를 합쳐서 ALS 알고리즘을 거친 행렬에서 각각의 원소 행렬 하나씩을 추출하여 반복합니다.\n","    if input_user_id != train_id :                                       # 입력 데이터 인덱스 번호가 기본 데이터 인덱스 번호와 맞지 않다면(predict의 마지막에는 입력값이 존재하기 때문에 중복을 방지하기 위함)\n","      cos_similarity = CosineSimilarity(input_user_vector, train_vector) # 입력 데이터 행렬과 기본 데이터 행렬의 코사인 유사도를 구하고 cos_similarity 변수에 대입합니다.\n","      if cos_similarity > -1 :                                           # 코사인 유사도가 -1보다 크다면\n","        cos_similarity_matrix.append([cos_similarity, train_id])         # 코사인 유사도와 해당 인덱스 번호를 cos_similarity_matrix 변수에 추가합니다.\n","\n","  # 코사인 유사도를 기준으로 재정렬 한 뒤 상위 n개의 인덱스 번호를 추출합니다.\n","  cos_similarity_matrix.sort(reverse=True)                 # 코사인 유사도를 기준으로 재정렬합니다.\n","  arr_predict = [i[1] for i in cos_similarity_matrix[:10]] # cos_similarity_matrix 변수에 i 번째 리스트에서 2번째 인덱스의 값을 추출하여 arr_predict 변수에 저장합니다.\n","\n","  # 추출한 인덱스 번호를 이용하여 '관광 명소'와 '주소'를 출력으로 내보냅니다.\n","  result = df_copy.iloc[arr_predict, 0:2]\n","\n","  # return result\n","  # return arr_predict"],"metadata":{"id":"AVPVWSbeNTwR","executionInfo":{"status":"ok","timestamp":1650469837875,"user_tz":-540,"elapsed":4,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["# 모델 생성 - ③ 카운팅 데이터 기반 모델(문제점 개선)\n","- 문제점: 사용자의 입력과 유사한 데이터가 상위에 노출되지 않고 중위권에 노출됨.\n","- 연산 흐름\n"," 1. 입력 데이터 + 학습 데이터 ALS 행렬 연산 수행\n"," 2. 입력 데이터와 학습 데이터 간의 코사인 유사도 연산 수행\n"," 3. 코사인 유사도를 기준으로 상위 10개의 데이터(행 인덱스 번호, 행렬) 추출\n"," 4. 입력 데이터와의 태그 부합 개수를 기준으로 랭크 정렬\n"," 5. 상위 n 개의 관광지 명과 상세 주소 출력"],"metadata":{"id":"PKsB0kp-gZr0"}},{"cell_type":"code","source":["def RecommendedSystemsImprovedVersion(area_code, input_data) :\n","\n","  # 필요한 패키지를 임포트합니다.\n","  import pandas as pd\n","  import numpy as np\n","  import matplotlib.pyplot as plt\n","\n","  ############################################################\n","  # 코사인 유사도를 구하는 함수를 생성합니다.\n","  def CosineSimilarity(v1, v2) :\n","    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n","\n","  # Loss Function을 구현합니다.\n","  def LossFunction(C, R, predict, X, Y, r_lambda) :\n","    predict_error = np.square(R - predict)                                     # R의 행렬 요소에 predict행렬 요소를 각각 빼고 모든 요소를 제곱하여 predict_error 변수에 저장합니다.\n","    confidence_error = np.sum(C * predict_error)                               # C의 행렬 배열과 predict_error의 행렬 배열을 모두 합하여 confidence_error 변수에 저장합니다.\n","    regularzation = r_lambda * ((np.sum(np.square(X))) + np.sum(np.square(Y))) # X와 Y의 행렬 배열의 각 요소에 제곱을 한 뒤 모든 요소를 합하고 r_lambda(40)을 곱한 뒤 regularzation 변수에 저장합니다.\n","    total_loss = confidence_error + regularzation                              # confidence_error와 regularzation을 합하여 total_loss 변수에 저장합니다.\n","    return np.sum(predict_error), confidence_error, regularzation, total_loss\n","\n","  # Loss Function을 최적화 시키는 Optimizer를 생성합니다.\n","  def OptimizeTourist(X, Y, C, R, nu, nf, r_lambda) :  \n","    yT = np.transpose(Y) # Y에 대한 전치 행렬을 구하여 yT 변수에 저장합니다.\n","\n","    for u in range(nu) :                             # nu 수만큼 반복합니다. (1,096)\n","      Cu = np.diag(C[u])                             # C의 u번째 대각 원소를 추출하고 대각 행렬을 만들어내어 Cu 변수에 저장합니다.\n","      yT_Cu_y = np.matmul(np.matmul(yT, Cu), Y)      # yT, Cu 변수의 행렬 곱을 수행하고 난 뒤 결괏값과 Y 변수의 행렬 곱을 수행하고 yT_Cu_y 변수에 저장합니다.\n","      lI = np.dot(r_lambda, np.identity(nf))         # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","      yT_Cu_pu = np.matmul(np.matmul(yT, Cu), R[u])  # yT, Cu 배열의 행렬 곱을 수행한 결괏값과 P[u] 배열의 행렬 곱을 수행하고 yT_Cu_pu 변수에 저장합니다.\n","      X[u] = np.linalg.solve(yT_Cu_y + lI, yT_Cu_pu) # yT_Cu_y + lI 결괏값과 yT_Cu_pu의 연립방정식을 풉니다.\n","\n","  def OptimizeTag(X, Y, C, R, ni, nf, r_lambda) :\n","    xT = np.transpose(X) # X에 대한 전치 행렬을 구하여 xT 변수에 저장합니다.\n","  \n","    for i in range(ni) :                               # ni 수만큼 반복합니다. (22)\n","      Ci = np.diag(C[:, i])                            # C의 모든 행의 I번째 컬럼을 추출하여 Ci 변수에 저장합니다.\n","      xT_Ci_x = np.matmul(np.matmul(xT, Ci), X)        # xT, Ci의 행렬 곱을 수행한 결괏값과 X와 행렬 곱을 수행하고 xT_Ci_x 변수에 저장합니다.\n","      lI = np.dot(r_lambda, np.identity(nf))           # nf의 정방 단위행렬을 반환받고 람다와 내적 연산을 수행한 뒤 lI 변수에 저장합니다.\n","      xT_Ci_pi = np.matmul(np.matmul(xT, Ci), R[:, i]) # xT, Ci의 행렬 곱을 수행한 결괏값과 P의 모든 행의 i번째 컬럼 원소값과 행렬곱을 수행하고 xT_Ci_pi 변수에 저장합니다.\n","      Y[i] = np.linalg.solve(xT_Ci_x + lI, xT_Ci_pi)   # xT_Ci_x + lI 결괏값과 xT_Ci_pi 연립방정식을 풉니다.\n","\n","  ############################################################\n","\n","  # 시트 순서 (전체 관광지[0]), (제주시, 추자면[1]), (초전읍, 구좌읍, 우도[2]), (한경면, 한림읍, 애월읍[3]), (서귀포시 남원읍[4]), (성산읍, 표선면[5]), (안덕면, 대정읍[6]) (테스트[7])\n","  df = pd.read_excel('/content/drive/MyDrive/Project/여행 코스 추천/data/학습용 데이터/tourist_data_learning.xlsx', index_col=0, sheet_name=area_code)\n","\n","  # 사본을 생성합니다.\n","  df_copy = df.copy()\n","\n","  # 필요없는 컬럼을 제거합니다.\n","  df.drop(['주소', '관광 명소'], axis=1, inplace=True)\n","\n","  # 관광지 정보 데이터와 사용자로 하여금 입력받은 데이터를 이어붙입니다.\n","  train_data = pd.concat([df, input_data])\n","\n","  # 학습 파라미터를 초기화 합니다. 초기값은 논문에서 가장 좋은 성능을 낸다는 값으로 설정합니다.\n","  r_lambda = 40 # 데이터 정규화에 필요한 변수입니다.\n","  nf = 200      # Matrix Factorization 학습 시에 정하는 임의의 차원 수이며 보통 50에서 200 사이로 설정합니다.\n","  alpha = 40    # Confidence level(신뢰도)을 조정할때 쓰일 변수입니다.\n","\n","  # 학습시킬 태그 데이터 행렬을 생성합니다. 행(row)은 관광지 수, 열(column)은 태그의 개수가 됩니다.\n","  R = train_data.iloc[:, :].values # train_data의 모든 행의 모든 열의 원소 값만 추출하여 R 변수에 저장합니다.\n","  P = np.copy(R)                   # 입력 태그의 부합하는 수를 세기 위해 R의 사본을 생성합니다.\n","\n","  # 아주 작은 랜덤 한 값들로 행렬의 값을 초기화시킵니다.\n","  nu = R.shape[0] # 관광지의 개수\n","  ni = R.shape[1] # 태그의 개수\n","\n","  # 행렬의 값을 아주 작은 값으로 초기화합니다.(rand: 0 ~ 1사이의 표준정규분포 난수 매트릭스를 생성합니다.)\n","  X = np.random.rand(nu, nf) * 0.01 # 관광지 수 매트릭스\n","  Y = np.random.rand(ni, nf) * 0.01 # 태그 수 매트릭스\n","\n","  # 학습용 태그 테이블에 Confidence level을 적용한 C 행렬을 구합니다.\n","  C = 1 + alpha * R\n","\n","  # 가중치를 조절하며 loss가 수렴되는 최종 ALS 행렬을 구합니다.\n","  for i in range(17) :\n","  \n","    if i != 0 :\n","      OptimizeTourist(X, Y, C, R, nu, nf, r_lambda) # optimize_tourist 함수를 불러와 loss를 구하고 가중치를 조절합니다.\n","      OptimizeTag(X, Y, C, R, ni, nf, r_lambda)     # optimize_tag 함수를 불러와 loss를 구하고 가중치를 조절합니다.\n","  \n","    predict = np.matmul(X, np.transpose(Y))\n","\n","  predict = np.matmul(X, np.transpose(Y))\n","\n","  # 코사인 유사도 함수를 활용하여 ALS 알고리즘을 거친 기본 데이터와 입력 데이터의 행렬을 나누어 입력 데이터 행렬에 가장 유사한 값을 기본 데이터 행렬에서 찾아 추출합니다.\n","  input_user_id = predict.shape[0] - 1         # predict.shape ex: (206, 22) [0] 번째에 해당하는 206에서 1을 뺀 값을 input_id 변수에 넘겨줍니다.(인덱싱에서 1번째 행은 0번 인덱스 번호를 가지기 때문)\n","  input_user_vector = predict[[input_user_id]] # predict행렬에 input_id번째 행렬 배열을 input_user_vector 변수에 전달합니다.\n","  best_match_score = -1                        # 코사인 유사도는 -1 ~ 1 사이의 값을 가지므로 최소값인 -1로 변수를 초기화합니다.\n","  best_match_id = -1                           # 코사인 유사도는 -1 ~ 1 사이의 값을 가지므로 최소값인 -1로 변수를 초기화합니다.\n","  best_match_matrix = []                       # 코사인 유사도로 가장 유사한 데이터의 행렬을 받을 변수입니다.\n","  cos_similarity_matrix = []                   # 입력 데이터와 기본 데이터 행렬의 코사인 유사도와 인덱스 번호를 받을 리스트 변수입니다.\n","\n","  # 반복문을 통하여 입력 데이터 행렬을 기본 데이터 행렬과 비교하여 가장 유사한 값을 찾습니다.\n","  for train_id, train_vector in enumerate(predict) :                     # 기본 데이터와 입력 데이터를 합쳐서 ALS 알고리즘을 거친 행렬에서 각각의 원소 행렬 하나씩을 추출하여 반복합니다.\n","    if input_user_id != train_id :                                       # 입력 데이터 인덱스 번호가 기본 데이터 인덱스 번호와 맞지 않다면(predict의 마지막에는 입력값이 존재하기 때문에 중복을 방지하기 위함)\n","      cos_similarity = CosineSimilarity(input_user_vector, train_vector) # 입력 데이터 행렬과 기본 데이터 행렬의 코사인 유사도를 구하고 cos_similarity 변수에 대입합니다.\n","      if cos_similarity > -1 :                                           # 코사인 유사도가 -1보다 크다면\n","        cos_similarity_matrix.append([cos_similarity, train_id])         # 코사인 유사도와 해당 인덱스 번호를 cos_similarity_matrix 변수에 추가합니다.\n","\n","  # 코사인 유사도를 기준으로 재정렬 한 뒤 상위 n개의 인덱스 번호를 추출합니다.\n","  cos_similarity_matrix.sort(reverse=True)                 # 코사인 유사도를 기준으로 내림차순으로 정렬합니다.\n","  arr_predict = [i[1] for i in cos_similarity_matrix[:10]] # cos_similarity_matrix 변수에 i 번째 리스트에서 1번째 인덱스의 값을 추출하여 arr_predict 변수에 저장합니다.\n","  arr_predict.insert(0, R.shape[0]-1)                      # arr_predict 리스트 0번째 자리에 R의 행 크기의 -1을 뺀 값 즉, 마지막 데이터의 인덱스 번호를 삽입합니다.\n","\n","  # 입력 태그에 부합하는 태그 개수를 세어 출력 우선 순위를 정합니다.\n","  P[P > 0] = 1                      # 곱셈 연산으로 입력 태그에 부합하지 않는 값을 제외 하기 위해 P의 원소값 중 0보다 큰 값의 경우 1로 치환합니다. ([0, 2, 1, 0] -> [0, 1, 1, 0])\n","  count_list = []                   # 입력 태그의 부합하는 개수와 해당 인덱스 번호를 받을 리스트 변수를 생성합니다.\n","  input_data = np.array(input_data) # 시리즈 타입을 넘파이 배열 타입으로 변경합니다.\n","\n","  for i in arr_predict[1:] :          # 추천 시스템에서 반환받은 인덱스 번호를 하나씩 반환하며 반복합니다. (첫 번째에는 입력 데이터가 있기 때문에 두 번째부터 시작합니다.)\n","    count = np.sum(P[i] * input_data) # P의 [i] 번째 배열과 입력 데이터의 배열을 곱한 뒤 모두 합합니다. 입력과 학습 데이터 둘 다 1이 아니라면 곱셈 연산으로 0이 되기 때문에 카운트에 포함되지 않습니다.\n","    count_list.append([count, i])     # 부합하는 태그 개수와 인덱스 번호를 count_list 변수에 추가합니다.\n","\n","  # 입력 태그에 부합하는 개수가 똑같은 값의 경우 내적 연산을 통해 우선 순위를 정합니다.\n","  rank_dict = {} # 부합 태그의 개수와 행 인덱스 번호를 받을 딕셔너리 변수를 생성합니다.\n","\n","  for i in count_list :          # count_list 변수에 저장된 리스트 개수를 하나씩 넘겨주며 반복합니다.\n","    rank_dict[i[0]] = []         # 리스트의 0번째(2.0, 13 -> 2.0)를 딕셔너리의 key값으로 사용하고 value 값으로 빈 리스트를 선언합니다.(ex: {} - > {1: [], 2: []})\n","  for i in count_list :          # count_list 변수에 저장된 리스트 개수를 하나씩 넘겨주며 반복합니다.\n","    rank_dict[i[0]].append(i[1]) # 리스트의 0번째 딕셔너리 변수에 value 값을 추가합니다. (ex: {2: []} -> {2: [1, 5, 14]})\n","\n","  input_user_id = arr_predict[0]             # 첫 번째 데이터는 입력값으로 입력값을 input_user_id 변수에 넘겨줍니다.\n","  input_user_vector = predict[input_user_id] # predict 행렬에 input_user_id 번째 행렬 배열을 input_user_vector 변수에 전달합니다.\n","\n","  for key in rank_dict.keys() :                                     # rank_dict 변수의 key 값을 넘겨주며 반복합니다.\n","    dot_product_result_val = []                                     # 내적 연산을 수행한 뒤 행렬과 인덱스 번호를 받을 리스트 변수입니다.\n","\n","    if len(rank_dict[key]) > 1 :                                    # rank_dict[key]에 value 개수가 1개 이상이면\n","      idx = rank_dict[key]                                          # idx 변수에 rank_dict[key] 모든 value를 넘겨줍니다.\n","\n","      for i in idx :                                                # idx 변수에 저장된 인덱스 번호를 하나씩 넘겨주며 반복합니다.\n","        train_vector = predict[i]                                   # predict에 i번째 행렬값을 train_vector 변수에 넘겨줍니다.\n","\n","        if input_user_id != i :                                     # 입력값 인덱스 번호가 기본 데이터 인덱스 번호와 같지 않다면(중복 방지)\n","          dot_product_val = np.dot(input_user_vector, train_vector) # 내적 연산을 수행합니다.\n","\n","          if dot_product_val > 0 :                                  # 내적 연산을 수행한 값이 0보다 크다면\n","            dot_product_result_val.append([dot_product_val, i])     # dot_product_result_val 변수에 내적 연산을 수행한 결괏값과 기본 데이터의 인덱스 번호를 추가합니다.\n","            dot_product_result_val.sort(reverse=True)               # 내림차순으로 정렬합니다.\n","            arr_dok = [j[1] for j in dot_product_result_val]        # dot_product_result_val 변수에 저장된 값 중 [n]번째 [1]번 인덱스 값을 추출하여 리스트 형태로 전달합니다. ([5.234, 3] → 3 추출 → arr_dok: [3, 1, 6, 3])\n","            rank_dict[key] = arr_dok                                # rank_dict 변수에 [j] key에 해당하는 value 값을 arr_dok 변수에 저장된 리스트 값으로 바꿉니다.\n","\n","  result = [] # 입력 태그의 부합하는 개수가 많은 장소 순으로 재배열한 최종 인덱스 번호를 받을 리스트 변수입니다.\n","\n","  # 조건문의 조건으로 사용할 딕셔너리의 키 값 중 가장 큰 값을 찾습니다.\n","  start_num = list(rank_dict)     # 딕셔너리 변수의 key 값을 리스트 형태로 start_num 변수에 전달합니다.\n","  start_num = int(max(start_num)) # start_num 변수에 저장된 값 중 가장 큰 수를 찾고 int형으로 변환한 뒤 start_num 변수를 초기화합니다.\n","\n","  for i in range(start_num, 0, -1) : # 딕셔너리의 key의 개수를 시작으로 0까지 -1씩 작아지며 반복합니다. (ex: 2, 0, -1)\n","    try :                                        # key가 맞지 않은 값이 들어올 경우를 대비해 예외 처리를 합니다. 오류가 발생하지 않을 경우 실행문을 실행합니다.\n","      result.append(rank_dict[i])                # rank_dict[i] value를 result 변수에 추가합니다.\n","    except :                                     # key가 맞지 않아 오류가 발생할 경우\n","      continue                                   # 해당 횟수는 넘어가고 다음 횟수 반복을 진행합니다.\n","\n","  result = sum(result, []) # 2차원 리스트를 1차원 리스트로 풀어줍니다. (ex: [[1, 3], [1, 2, 4]] → ([] + [1, 3] + [1, 2, 4]) → [1, 3, 1, 2, 4])\n","                                                                              # 2차원 리스트           리스트 연산 과정             연산 결과\n","  # 추출한 인덱스 번호를 이용하여 '관광 명소'와 '주소'를 출력으로 내보냅니다.\n","  result = df_copy.iloc[result, 0:2]\n","\n","  return result"],"metadata":{"id":"Et1GPRMMhWpM","executionInfo":{"status":"ok","timestamp":1650474239917,"user_tz":-540,"elapsed":925,"user":{"displayName":"박진완","userId":"11571407450925957575"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":["# 비교 확인\n","print('부합 태그 수 딕셔너리: {}'.format(rank_dict1))\n","print('-'*100)\n","print(idx)\n","print('-'*100)\n","print(dot_product_result_val)\n","print('-'*100)\n","print(dot_product_result_val)\n","print('-'*100)\n","print(arr_dok)\n","print('-'*100)\n","print('태그 수 같은 것끼리 내적 연산 후 딕셔너리: {}'.format(rank_dict2))\n","print('-'*100)\n","print(dim1_result)\n","print('-'*100)\n","print(start_num)"],"metadata":{"id":"aglVQAlfCSwb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650474012266,"user_tz":-540,"elapsed":382,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"9aa9d5d3-3937-48d3-b772-7a36ae987136"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["부합 태그 수 딕셔너리: {4.0: [140, 154, 101], 2.0: [0, 86, 41, 30], 3.0: [193, 7], 1.0: [107]}\n","----------------------------------------------------------------------------------------------------\n","[193, 7]\n","----------------------------------------------------------------------------------------------------\n","[[8.206666329166447, 101], [7.814529934595919, 7], [6.339655208374089, 140], [6.336388963361358, 86], [6.336388963361358, 41], [6.336388963361358, 30], [6.06672548594557, 0], [5.869432681220265, 193], [5.8115632392176675, 154]]\n","----------------------------------------------------------------------------------------------------\n","[[8.206666329166447, 101], [7.814529934595919, 7], [6.339655208374089, 140], [6.336388963361358, 86], [6.336388963361358, 41], [6.336388963361358, 30], [6.06672548594557, 0], [5.869432681220265, 193], [5.8115632392176675, 154]]\n","----------------------------------------------------------------------------------------------------\n","[101, 7, 140, 86, 41, 30, 0, 193, 154]\n","----------------------------------------------------------------------------------------------------\n","태그 수 같은 것끼리 내적 연산 후 딕셔너리: {5.0: [251, 836], 4.0: [654, 844, 894], 3.0: [112, 606, 1035, 1091], 2.0: [889]}\n","----------------------------------------------------------------------------------------------------\n","[654, 844, 894, 112, 606, 1035, 1091, 889]\n","----------------------------------------------------------------------------------------------------\n","[5.0, 4.0, 3.0, 2.0]\n"]}]},{"cell_type":"code","source":["input_data = pd.read_excel('/content/drive/MyDrive/Project/여행 코스 추천/data/학습용 데이터/test.xlsx', sheet_name=3)\n","# result = RecommendedSystems(1, input_data)  # 카운팅 데이터 모델\n","# result = RecommendedSystems1(1, input_data) # 전체 1인 데이터 모델\n","result = RecommendedSystemsImprovedVersion(0, input_data)\n","# display(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rd6oCZk4cNtl","executionInfo":{"status":"ok","timestamp":1650474284809,"user_tz":-540,"elapsed":43036,"user":{"displayName":"박진완","userId":"11571407450925957575"}},"outputId":"a1e44050-580c-454d-b817-74e2b1a65ffd"},"execution_count":121,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n","  # This is added back by InteractiveShellApp.init_path()\n"]}]},{"cell_type":"markdown","source":["# 참고 사이트\n","- https://yeomko.tistory.com/3"],"metadata":{"id":"L2QaK8iNN9QO"}},{"cell_type":"markdown","source":["# 사용하지 않는 코드\n"],"metadata":{"id":"Zl9-S70kvQLC"}},{"cell_type":"code","source":["  #########################내적 연산#########################\n","  # # 내적 연산은 통해 사용자가 선택한 테마에 태그가 더 많은 장소를 우선적으로 추천합니다\n","  # input_user_id = arr_predict[0]                            # 입력값의 인덱스 번호를 input_id 변수에 넘겨줍니다.\n","  # input_user_vector = predict[arr_predict[0]]               # 입력값의 ALS 행렬을 input_vector 변수에 넘겨줍니다.\n","  # best_match, best_match_id, best_match_vector = -1, -1, [] \n","  # similar_user_dot_predict = []                             # 내적 연산을 수행한 뒤 행렬과 인덱스 번호를 받을 리스트 변수입니다.\n","  \n","  # for x in arr_predict :                                        # arr_predict 변수에 저장된 인덱스 번호 개수 만큼 반복합니다.\n","  #   train_id = x                                                # arr_predict 변수에 저장된 인덱스 번호를 train_id 변수에 저장합니다.\n","  #   train_vector = predict[x]                                   # predict에 x번째 행렬값을 train_vector 변수에 넘겨줍니다.\n","  #   if input_user_id != train_id :                              # 입력값 인덱스 번호가 기본 데이터 인덱스 번호와 같지 않다면(중복 방지)\n","  #     similarity = np.dot(input_user_vector, train_vector)      # 내적 연산을 수행합니다.\n","  #     if similarity > 0 :                                       # 내적 연산을 수행한 값이 0보다 크다면\n","  #       similar_user_dot_predict.append([similarity, train_id]) # similar_user_dot_predict 변수에 내적 연산을 수행한 결괏값과 기본 데이터의 인덱스 번호를 추가합니다.\n","\n","  # similar_user_dot_predict.sort(reverse=True)        # 내적 연산을 수행항 값을 기준으로 내림차순으로 정렬합니다.\n","  # arr_dot = [i[1] for i in similar_user_dot_predict] # 인덱스 번호를 추출하여 arr_dot 리스트 변수에 넘겨줍니다.\n","  ###########################################################"],"metadata":{"id":"ucu_P0OHvRms"},"execution_count":null,"outputs":[]}]}